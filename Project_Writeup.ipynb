{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term 1 Project 3 : Behavioral Cloning\n",
    "#### Project Writeup\n",
    "\n",
    "##### Behavioral Cloning Project\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "* Use the simulator to collect data of good driving behavior\n",
    "* Build, a convolution neural network in Keras that predicts steering angles from images\n",
    "* Train and validate the model with a training and validation set\n",
    "* Test that the model successfully drives around track one without leaving the road\n",
    "* Summarize the results with a written report\n",
    "---\n",
    "#### Rubric Points\n",
    "Here I will consider the [rubric points](https://review.udacity.com/#!/rubrics/432/view) individually and describe how I addressed each point in my implementation\n",
    "\n",
    "---\n",
    "### Files Submitted & Code Quality\n",
    "\n",
    "#### 1. Submission includes all required files and can be used to run the simulator in autonomous mode\n",
    "\n",
    "My project includes the following files:\n",
    "\n",
    "|File Name        |Purpose                                                                             |\n",
    "|:----------------|------------------------------------------------------------------------------------|\n",
    "|model.py         |Contains the script for the model, pre-processing and augmentation of data, training|\n",
    "|drive.py         |Used for driving the car in autonomous mode using the model generated by model.py   | \n",
    "|model.h5         |Contains a trained convolution neural network                                       |\n",
    "|video.mp4        |Autonomous run in the simulator for Track 1 based on the model generated by model.py|\n",
    "|writeup_report.md|Contains project writeup                                                            |   \n",
    "\n",
    "#### 2. Submission includes functional code\n",
    "Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing \n",
    "```sh\n",
    "python drive.py model.h5\n",
    "```\n",
    "\n",
    "#### 3. Submission code is usable and readable\n",
    "\n",
    "The model.py file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works.\n",
    "\n",
    "---\n",
    "### Model Architecture and Training Strategy\n",
    "\n",
    "#### 1. Model Architecture\n",
    "My model is based on the NVIDIA End-End Deep Learning for Autonomous Cars.\n",
    "I've modified the architecture to fit to my use case and tuned it for better results.\n",
    "\n",
    "A pictorial representation of the model can be seen below\n",
    "\n",
    "![Behavioral Cloning Model Architecture](ModelArchitecture.png)\n",
    "\n",
    "I've built my model using Keras as it defines simple syntax for defining models so that I can concentrate more on pre-processing and augmenting the data.\n",
    "\n",
    "Here's how my model is a good fit for this model\n",
    "\n",
    " **Input Layer** : It all starts with a 160 * 320 pixel Color Image fed to the network. The training data set is generated using the simulator training environment's video feed\n",
    "\n",
    " **Normalization Layer** : I've used Keras Lamdba layer to normalize the input. Normalizing and making the data zero-mean makes the model to train much faster as the gradient descent can be handled in a much better way for zero-mean numbers\n",
    "\n",
    " **Convolutional Layers** : Since we are dealing with images, the feautre classification is important to identify the corners and take decisions based on that. Convolutional layers help a lot for this problem.\n",
    "\n",
    " **Dropout Layers** : Overtraining the model could lead to over-fitting and this can be avoided by randomly dropping certain weights using dropout layers. This will help the model to train in such a way that it is not dependent on any set of parameters as the parameters can become zero anytime - thus decoupling the hyperparameters from the results\n",
    "\n",
    " **Fully Connected Layers** : The problem of identifying the steering angle is a regression problem and Fully Connected layers help in achieving the right angle based on the training model\n",
    "\n",
    " **Activations** : The model should be non-linear and in order to achieve that, ELU functions have been used for activating the neurons\n",
    " \n",
    " **Optimizer** : Adam (Adaptive Moment Estimation) Optimization is used during training. This is an extention to the Stochastic Gradient Descent method. In this method, instead of having a single learning rate during gradient descent, the algorithm computes individual adaptive learning rates for different parameters \n",
    "\n",
    "Here's a representation of the model with hyperparmeters in each layer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "____________________________________________________________________________________________________\n",
    "Layer (type)                     Output Shape          Param #     Connected to                     \n",
    "====================================================================================================\n",
    "lambda_1 (Lambda)                (None, 160, 320, 3)   0           lambda_input_1[0][0]             \n",
    "____________________________________________________________________________________________________\n",
    "cropping2d_1 (Cropping2D)        (None, 65, 320, 3)    0           lambda_1[0][0]                   \n",
    "____________________________________________________________________________________________________\n",
    "convolution2d_1 (Convolution2D)  (None, 31, 158, 24)   1824        cropping2d_1[0][0]               \n",
    "____________________________________________________________________________________________________\n",
    "convolution2d_2 (Convolution2D)  (None, 14, 77, 36)    21636       convolution2d_1[0][0]            \n",
    "____________________________________________________________________________________________________\n",
    "convolution2d_3 (Convolution2D)  (None, 5, 37, 48)     43248       convolution2d_2[0][0]            \n",
    "____________________________________________________________________________________________________\n",
    "dropout_1 (Dropout)              (None, 5, 37, 48)     0           convolution2d_3[0][0]            \n",
    "____________________________________________________________________________________________________\n",
    "convolution2d_4 (Convolution2D)  (None, 3, 35, 64)     27712       dropout_1[0][0]                  \n",
    "____________________________________________________________________________________________________\n",
    "convolution2d_5 (Convolution2D)  (None, 1, 33, 64)     36928       convolution2d_4[0][0]            \n",
    "____________________________________________________________________________________________________\n",
    "dropout_2 (Dropout)              (None, 1, 33, 64)     0           convolution2d_5[0][0]            \n",
    "____________________________________________________________________________________________________\n",
    "flatten_1 (Flatten)              (None, 2112)          0           dropout_2[0][0]                  \n",
    "____________________________________________________________________________________________________\n",
    "dense_1 (Dense)                  (None, 512)           1081856     flatten_1[0][0]                  \n",
    "____________________________________________________________________________________________________\n",
    "dense_2 (Dense)                  (None, 64)            32832       dense_1[0][0]                    \n",
    "____________________________________________________________________________________________________\n",
    "dropout_3 (Dropout)              (None, 64)            0           dense_2[0][0]                    \n",
    "____________________________________________________________________________________________________\n",
    "dense_3 (Dense)                  (None, 10)            650         dropout_3[0][0]                  \n",
    "____________________________________________________________________________________________________\n",
    "dense_4 (Dense)                  (None, 1)             11          dense_3[0][0]                    \n",
    "====================================================================================================\n",
    "Total params: 1,246,697\n",
    "Trainable params: 1,246,697\n",
    "Non-trainable params: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Training & Testing\n",
    "This essence of this project is to determine the steering angle based on the position of the center of the car with respect to its surroundings. This is a regression problem wherein the model should evaluate the best possible angle to feed in to the steering of the car. This model is derived based on training data created in a similar(need not be same) environment as test data\n",
    "\n",
    "**Generating Training Data**\n",
    "The training data is derived by driving the car in Udacity's Self Driving Car Simulator.\n",
    "\n",
    "User is required to drive the car using keyboard controls trying to be in the center of the lane. This video feed is converted to pictures and the names of the pictures are associated steering angles at each frame.\n",
    "\n",
    "The car has 3 cameras in the simulator, so each frame provides us with 3 different views of hte same image - a center image, an image from the left side of the windsheild, an image from the right side of the wind sheild.\n",
    "\n",
    "The steering angle will always be the angle based on the center of the car. This angle can be manipulated to get the corresponding angles for left and right images\n",
    "\n",
    "Note : Though the simulator lets you drive the car, I could not drive the car smoothly using my keyboard. I was unable to avoid crossing the side lanes multiple times. The model trained on the testing data I generated was not doing so great - as the training data itself was not proper. Hence I've decided to use the Udacity provided trained data. The results obtained with this training data was much better, and my car was always within the lane boundaries.\n",
    "\n",
    "**Pre-Processing & Augmenting the Training Data**\n",
    "Udacity's training data consisted of ~8500 images. These are the center, left & right images of each frame during the course of training set generation. Though the model can be trained based on this model, inorder to make the model more generic and adaptable to any environment, i've pre-processed and augmented the data\n",
    "\n",
    "**Pre-processing**\n",
    "- **Conversion of BGR to RGB** : This comes under pre-processing step. CV2 reads the image in BGR colorspace. But the drive.py which is used to test the model in the simulator reads the image in RGB colorspace. Hence the images are converted to RGB colorspace as soon as they are read\n",
    "\n",
    "    Code Snippet `cv2.cvtColor(center_image_bgr, cv2.COLOR_BGR2RGB)`\n",
    "    \n",
    "- **Cropping the Image** : The image given by the camera in the simulator consists of the Car's dashboard at the bottom as well as the sky and trees at the top. These values do not really contribute anything for the model's decision. Hence to avoid considering these areas, the images are cropped. This is done in the Model itself rather than in the pre-processing pipeline, but to have a view on what's hapenning, here's the representation\n",
    "\n",
    "    Code Snippet `model.add(Cropping2D(cropping=((70, 25), (0, 0)))) #70 is pixels from top and 25 is pixels from bottom`\n",
    "\n",
    "- **Gaussian Blur** : This comes under pre-processing step. Inorder to avoid sudden spikes in the color ranges and to introduce normalization, Gaussian Blur is applied to the images\n",
    "\n",
    "    Code Snippet `cv2.GaussianBlur(left_image, (3, 3), 0) #(3, 3) represents the kernel size while applying blur`\n",
    "\n",
    "**Data Augmentation**\n",
    "- **Image Translation** : To provide better training samples, each image will be translated laterally at a random factor, and the corresponding steering angle is calculated. This step will help the model to get more training samples with different angles to better train the model\n",
    "\n",
    "    Code Snippet `cv2.warpAffine(X[i], trans_m, (width, height)) #trans_m is a matrix with the amount of translation required`\n",
    "    \n",
    "- **Image Flip** Note that the training data is based on a loop that is left aligned. If we train the model based on this data, the model will tend to move to the left mostly. Inorder to avoid this, for each image where the angle is greater than a particular value(which represents a curve), we flip the image and add to the training data. This leads to a generalized training set, and more number of training data\n",
    "\n",
    "    Code Snippet `cv2.flip(image, 1) #1 represents flipping horizontally, and steering angles will be multiplied by -1`\n",
    "    \n",
    "- **Changing Brightness** The training data is also augmented by randomly changing the brightness of the image and adding to the training set. This gives us with more training data, with images representing different lighting conditions. This makes the model more generalized and can be adapted to different tracks and lighting conditions\n",
    "\n",
    "    Code Snippet `hsv_img[:,:,2] =  hsv_img[:,:,2] * random_value #Image is converted to HSV channel where the last channel (V - Value) represents brightness`\n",
    "\n",
    "Note : I've uesd Pre-processing steps and replaced the original images. Whereas during augmentation, i've added the transformed images to the existing training set which inturn increased the number of training samples\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
